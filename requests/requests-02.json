[
    {
        "article": "articles/Attention-Is-All-You-Need.md",
        "query": "In what ways does the multi-head attention mechanism allow the model to jointly attend to information from different representational subspaces at various positions?"
    },
    {
        "article": "articles/Attention-Is-All-You-Need.md",
        "query": "training methodologies and hardware configurations"
    },
    {
        "article": "articles/The-Linear-Representation-Hypothesis.md",
        "query": "Summarize the linear representation hypothesis and its implications for neural network interpretability."
    },
    {
        "article": "articles/Forget-What-You-Know-about-LLMs.md",
        "query": "Model scale as an indicator of overfitting sensitivity."
    },
    {
        "article": "articles/Forget-What-You-Know-about-LLMs.md",
        "query": "Summarize the article's findings on the comparative resilience of different LLM families, such as Llama versus others, to textual perturbations."
    },
    {
        "article": "articles/Hierarchical-Reasoning-Model.md",
        "query": "Explain the hierarchical reasoning model and its significance in understanding complex systems."
    }
]