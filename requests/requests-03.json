[
    {
        "article": "articles/Attention-Is-All-You-Need.md",
        "query": "Enhancing representational power through parallel, subspace-specific attention."
    },
    {
        "article": "articles/Attention-Is-All-You-Need.md",
        "query": "The role of regularization and optimization schemes in training large-scale models."
    },
    {
        "article": "articles/Evolutionary Computation LLM.md",
        "query": "The LLM as a substitute for evolutionary operators."
    },
    {
        "article": "articles/Evolutionary Computation LLM.md",
        "query": "The article's perspective on the theme of black-box optimization."
    },
    {
        "article": "articles/Evolutionary Computation LLM.md",
        "query": "The application of multi-objective evolutionary principles throughout the surveyed research."
    },
    {
        "article": "articles/Evolutionary Computation LLM.md",
        "query": "The dual role of the LLM-EA synergy in the context of model and code security."
    },
    {
        "article": "articles/Evolutionary Computation LLM.md",
        "query": "Neural Architecture Search (NAS) as both a target for optimization and a domain for synergy."
    },
    {
        "article": "articles/Forget-What-You-Know-about-LLMs.md",
        "query": "Differential robustness across distinct LLM architectural families."
    },
    {
        "article": "articles/Forget-What-You-Know-about-LLMs.md",
        "query": "The methodology of using generative models to create adversarial evaluation datasets."
    },
    {
        "article": "articles/Forget-What-You-Know-about-LLMs.md",
        "query": "The paper's critique of a leaderboard-centric evaluation culture in NLP."
    },
    {
        "article": "articles/Hierarchical-Reasoning-Model.md",
        "query": "Emergent dimensionality hierarchy in a trained reasoning model as a parallel to the functional organization of the mouse cortex."
    },
    {
        "article": "articles/Hierarchical-Reasoning-Model.md",
        "query": "Applying principles of hierarchical processing, temporal separation, and local credit assignment from neuroscience to engineer stable, deep reasoning in artificial neural networks."
    },
    {
        "article": "articles/Learn Beyond The Answer.md",
        "query": "The role and limitations of proprietary expert models in synthetic data generation for LLM research."
    },
    {
        "article": "articles/Learn Beyond The Answer.md",
        "query": "A framework for categorizing mathematical reasoning skills in language models, from basic forward reasoning to complex reflective capabilities."
    },
    {
        "article": "articles/Learn Beyond The Answer.md",
        "query": "The interplay between data quantity, quality, and complexity in fine-tuning language models for mathematical tasks."
    },
    {
        "article": "articles/MemOS.md",
        "query": "The framework for accountable and governed memory in multi-agent systems."
    },
    {
        "article": "articles/MemOS.md",
        "query": "Mechanisms for the lifecycle and cross-type transformation of memory."
    },
    {
        "article": "articles/MemOS.md",
        "query": "Parallels between the MemOS architecture and cognitive models of memory."
    },
    {
        "article": "articles/Random Teachers are Good Teachers.md",
        "query": "The asymmetric nature of the loss landscape around a random initialization."
    },
    {
        "article": "articles/Random Teachers are Good Teachers.md",
        "query": "How random teacher distillation pre-conditions a network for supervised training."
    },
    {
        "article": "articles/Reinforcement Learning for Reasoning in Small LLMs.md",
        "query": "Causes of optimization instability and performance degradation during RL fine-tuning of small language models."
    },
    {
        "article": "articles/Reinforcement Learning for Reasoning in Small LLMs.md",
        "query": "The impact of output length constraints on the stability and efficacy of training LLMs for complex reasoning tasks."
    },
    {
        "article": "articles/Reinforcement Learning for Reasoning in Small LLMs.md",
        "query": "The challenge of managing multilingual language drift when fine-tuning a foundational model for a monolingual reasoning task."
    },
    {
        "article": "articles/Rethinking Training Signals in RLVR.md",
        "query": "The influence of a model's pretrained latent abilities (like code reasoning) on the outcomes of reinforcement learning, irrespective of the reward signal's quality."
    },
    {
        "article": "articles/Rethinking Training Signals in RLVR.md",
        "query": "A critique of model-specific conclusions in RL research, using the Qwen model family as a case study for how easily performance gains can be achieved with spurious signals."
    },
    {
        "article": "articles/Task Singular Vectors.md",
        "query": "The geometric interpretation of task interference through singular vector alignment."
    },
    {
        "article": "articles/Task Singular Vectors.md",
        "query": "An argument for structure-aware model manipulation over flattened parameter approaches."
    },
    {
        "article": "articles/Task Singular Vectors.md",
        "query": "The application of Orthogonal Procrustes analysis to decorrelate task-specific feature bases."
    },
    {
        "article": "articles/The-Linear-Representation-Hypothesis.md",
        "query": "The derivation of a semantically meaningful geometry from the statistical properties of the model's full vocabulary, rather than from its training data distribution."
    },
    {
        "article": "articles/The-Linear-Representation-Hypothesis.md",
        "query": "The article's use of counterfactuals as a formal language to ground and connect disparate intuitions about linear representations."
    },
    {
        "article": "articles/The-Linear-Representation-Hypothesis.md",
        "query": "Providing a single geometric framework that formally connects the subspace hypothesis (e.g., word2vec analogies) with the practical techniques of linear probing and activation steering."
    },
    {
        "article": "articles/distillation of SOTA embedding models.md",
        "query": "Methodology for fusing knowledge from heterogeneous teacher models."
    },
    {
        "article": "articles/distillation of SOTA embedding models.md",
        "query": "Self-distillation as a mechanism for post-hoc modality alignment."
    },
    {
        "article": "articles/rStar-Math.md",
        "query": "The role of code execution as a verifier for synthetic reasoning data."
    },
    {
        "article": "articles/rStar-Math.md",
        "query": "Novel training methodologies for process reward models that bypass noisy score annotation."
    }
]
